paper
```shell
https://arxiv.org/pdf/1706.03762.pdf
```

# 注意力就是你需要的所有

## 摘要

主导的序列转换模型是基于复杂的循环或卷积神经网络，包括编码器和解码器。最佳性能  
的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单的网络结构，  
即Transformer，它只是基于注意力机制，完全不需要循环和卷积。两个机器学习任务的  
实验表明，这些模型在质量上更优，同时更具并行性，训练时间明显更少。我们的模型  
在WMT 2014 English-to-German翻译任务中实现了28.4 BLEU，这比现有的最佳结果（包括  
集成）提高了2 BLEU以上。在WMT 2014 English-to-French 翻译任务中，我们的模型  
在8个GPU上培训了3.5天后建立了一个新的单一模型，即最先进的BLEU分数41.8,这只是  
文献中最佳模型训练成本的一小部分。结果表明，Transformer能很好的推广到其他任务中  
并成功的应用于大样本和有限样本的英语选取分析中。

## 1 介绍

循环神经网络，特别是长短期记忆和门控循环神经网络，作为一种最先进的序列建模和  
转换方法，例如语言建模和机器翻译，已经被牢固的建立起来。此后，许多工作都在  
继续推进循环神经网络语言模型和编码器-解码器体系结构的界限。循环模型通常是沿着  
输入和输出序列的符号位置进行因子计算。将位置与计算时间中的步骤对齐，它们生成  
一系列隐藏状态h<sub>t</sub>，作为先前隐藏状态h<sub>t-1</sub>和位置t输入的函数。这种固有的顺序性  
阻止了训练样本间的并行化，因为内存约束限制了跨样本的批处理，因此在较长的序列  
长度下，并行化显得至关重要。最近的工作已经通过因子分解技巧和条件计算显著提高  
计算效率，同时也提高了模型在后一种情况下的性能。然而，顺序计算的基本约束仍然存在。  

注意力机制已经成为各种任务中引人注目的序列建模和转换模型的一个组成部分，允许  
在不考虑其在输入或输出序列中的距离的情况下对依赖性进行建模。然而，在除少数  
情况外的所有情况下，这种注意力机制都通常与一个循环网络结合使用。在这项工作中，  
我们提出了Transformer，模型架构回避了循环，而是完全依赖一个注意力机制来绘制  
输入和输出之间的全局依赖关系。Transformer 允许更大程度的并行化，并且在8个P100 GPUs
上接受了12个小时的训练后，可以在翻译质量方面达到新的技术水品。

## 2 背景

减少顺序计算的目的也形成了Extended Neural GPU，ByteNet和ConvS2S的基础，所有这些  
都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。在这些  
模型中，将来自两个任意输入或输出的位置信号联系起来所需要的操作数量在位置之间的  
距离上增加，对于ConvS2S是线性的，对于ByteNet是对数的。这使得学习远距离之间的  
依赖性变得更加困难。在Transformer中，这减少到恒定的操作次数，尽管由于平均注意力  
加权位置而降低有效分辨率的成本，如第3.2节中所述，我们用Multi-Head Attention抵消  
这种影响。

Self-attention 有时被称为intra-attention，是一种注意力机制，为了计算一个序列的  
表示，它将该序列的不同位置联系起来。在阅读理解，抽象总结，文本蕴涵和学习任务独立  
的句子表达等多种任务中，人们都成功的运用了Self-attention。End-to-end memory network  
是基于一种循环注意力机制，而不是顺序一致的循环，并且在简单的语言问题解答和语言  
建模任务中表现良好。然而，据我们所知，Transformer是第一个完全依赖于self-attention  
来计算输入和输出表示的转换模型，而不使用sequence-aligned RNNs 或卷积。在下面的  
章节中，我们将描述Transformer，self-attention 动机，并讨论它相对于[17,18]和[9]  
等模型的优势。

## 3 模型架构

大多数有竞争力的神经序列转换模型都具有一个编码－解码结构。在这里，编码器将符号  
表示的输入序列（x<sub>1</sub>,...,x<sub>n</sub>)映射为连续表示序列  
**z** = (z<sub>1</sub>,...,z<sub>n</sub>)。 给定**z**,  
解码器然后一次生成一个符号的输出序列(y<sub>1</sub>,...,y<sub>m</sub>)。  
在每个步骤中，模型都是自动回归的，在生成下一个时，将先前生成的符号作为附加输入使用。  

Transformerz遵循这一总体架构，使用堆叠的self-attention和point-wise，编码器和  
解码器的全连接层，分别如图1的左半部分和右半部分。

图1：Transformer - 模型架构

## 3.1 编码器和解码器堆栈

**编码器** 编码器由一组n=6个相同层组成。每层有两个子层。第一层是一个  
multi-head self-attention mechanism,第二层是一个简单的position-wise fully  
connnected feed-forward network。我们在两个子层中的每个子层周围使用一个残差连接，  
然后进行层规范化。也就是说，每个子层的输出是 LayerNorm（x + Sublayer(x)),其中  
Sublayer(x) 是子层本身实现的函数。为了方便这些残差连接，模型中的所有子层以及  
嵌入层都会生成维度d<sub>model</sub> = 512的输出。

**解码器** 解码器同样由一组n=6个相同层组成。除了每个编码器中的两个子层外，  
解码器还插入第三个子层，该子层在编码器堆栈上的输出上执行multi-head attention。  
与编码器类似，我们在每一个子层周围使用残差连接，然后进行层规范化。我们还修改了  
解码器堆栈中的self-attention 子层，以防止后续位置参与。这种掩盖，联合输出  
嵌入被一个位置偏移的事实，确保了位置i的预测只能依赖于位置小于i时的已知输出。  

## 3.2 注意力

注意力函数可以描述为将一个查询和一组键值对儿映射到一个输出，其中查询，键，值和  
输出都是向量。输出以值的加权和计算，其中分配给每个值的权重是通过查询兼容函数和  
相应键计算的。

图2 （左）Scaled Dot-Product Attention。 (右) Multi-Head Attention 由一些并行  
运行的注意力层组成。

## 3.2.1 Scaled Dot-Product Attention

我们称我们的特别注意力“Scaled Dot-Product Attention"（图2）。输入包括维度  
d<sub>k</sub>的查询和键，以及维度d<sub>v</sub> 的值。我们用所有键计算查询的  
点积，将每个键除以d<sub>k</sub>，然后应用SoftMax函数获得值的权重。在实践中，  
我们同时计算一组查询上的注意力函数，将他们  
打包成矩阵Q。键和值也被打包成矩阵K和V。我们将输出矩阵计算为：  

Attention(Q,K,V) = softmax($\frac{QK^T}{\sqrt{d_k}}$)V  

最常用的注意力函数是additive attention 和 dot-product（multi-plicative）attention。  
Dot-product attention 与我们的算法是相同的，除了比例因子$\frac{1}{\sqrt{d_k}}$。  
Additive attention 使用具有单个隐藏层的前馈网络计算兼容性函数。虽然两者在理论上  
复杂度是相似的，但在实践中，dot-product attention速度更快，空间效率更高，因为它  
可以使用高度优化的矩阵乘法代码来实现。对于较小的d<sub>k</sub>值，这两种机制的  
性能相似，additive attention 优于dot product attention 而不缩放较大的d<sub>k</sub>值。  
我们怀疑，对于较大的d<sub>k</sub>值，dot products的增长幅度较大，将softmax函数  
推到具有极小梯度的区域。为了抵消这种影响，我们按$\frac{1}{\sqrt{d_k}}$缩放点积。  

## 3.2.2 Multi-Head Attention

我们发现，不使用d<sub>model</sub> 维键，值和查询来执行单注意力函数，而是使用  
不同的学习线性投影分别将查询，键和值h次线性投影到d<sub>k</sub>,d<sub>k</sub>和  
d<sub>v</sub>维度，这是有益的。在这些查询，键和值的每个投影版本上，然后我们并行  
执行注意力函数，生成d<sub>v</sub>维的输出值。如图所示，他们被连接起来，然后再次  
投影，从而得到最终的值。  

Multi-head attention 允许模型共同关关注来自不同位置的不同表示子空间的信息。  
平均值可以抑制单注意力头。

MultiHead(Q,K,V) = Concat(head<sub>1</sub>,...,head<sub>g</sub>)W<sup>O</sup>  
其中 head<sub>i</sub> = Attention($QW_i^Q,KW_i^K,VW_i^V$)

其中投影是参数矩阵 $W_i^Q ∈ R^{d_{model} × d_k}$, $W_i^K ∈ R^{d_{model} × d_k}$,  
$W_i^V ∈ R^{d_{model} × d_v}$ 和 $W^O ∈ R^{hd_{v} × d_{model}}$。

这这项工作中，我们使用了h=8的并行注意力层或者头。对于每一个模型，我们使用  
$d_k = d_v = d_{model}/h = 64$。由于每个头部的尺寸减小，总的计算成本与单头全尺寸  
注意力的计算成本相似。

## 3.2.3 我们模型中注意力的应用

Transformer 使用multi-head attention 以三种不同的方式。

    1. 在”编码器-解码器-注意“层，查询来自前一个解码器层，内存键和值来自编码器的输出。  
    这使得解码器中的每个位置都可以处理输入序列中的所有位置。这模仿了典型的编码器-解码器  
    注意力机制的顺序模型，如[38,2,9]。

    1. 编码器包含self-attention层。 在一个self-attention层中，所有键，值和查询  
    都来自同一个地方，在本例中，是编码器中前一层的输出。编码器中每个位置都可以  
    处理编码器前一层中的所有位置。

    1. 同样，解码器中的self-attention层允许解码器中每个位置关注解码器中的所有位置，  
        直到并包括该位置。为了保持自回归特性，需要防止解码器中出现向左的信息流。  
        我们通过屏蔽softmax输入中与非法连接相对应的所有值（设置为-∞), 在scaled  
        dot-product attention 内部实现了这一点。看图２

## 3.3 Position-wise Feed-Forward Networks

除了注意力子层外，我们的编码器和解码器的每个层都包含了一个完全连接的前馈网络，  
该网络分别应用于每个位置，并且完全相同。这包含连个线性变换带着ReLU激活在中间。  

FFN(x) = $max(0, xW_1 + b_1)W_2 + b_2$

虽然线性变换在不同的位置上是相同的，但他们在不同的层之间使用不同的参数。另一种  
描述这一点的方法是两个内核大小为1的卷积。输入和输出的维数为d<sub>model</sub>=512,  
内层维数为d<sub>ff</sub>=2048 。

## Embeddings 和　Softmax

与其他序列转换模型类似，我们使用学习的嵌入将输入标记和输出标记转换为d<sub>model</sub>维的向量。  
我们还使用通常学习的线性变换和SoftMax函数将解码器输出转换为预测下一个词的概率。  
在我们的模型中，我们在两个嵌入层之间共享相同的权重矩阵，并使用pre-softmax 线性  
变换，类似与[30].在嵌入层中，我们将这些权重乘以$\sqrt{d_{model}}$。  

## Positional Encoding

由于我们的模型不包含循环和卷积，为了使模型能够利用序列的顺序，我们必须注入一些  
关于序列中标记的相对或绝对位置的信息。为此，我们将”位置编码“添加到编码器和解码器  
堆栈的底部。位置编码与嵌入有相同的d<sub>model</sub> 维，因此可以将两者相加。位置  
编码有很多选择，学习的和固定的。  

在这项工作中，我们使用不同频率的正弦和余弦函数。  

$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$  
$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$  

pos 是位置，i是维度。也就是说位置编码的每个维度都对应于一个正弦曲线。波长形成  
2π到10000*2π的几何级数。我们之所以选择这个函数，是因为我们假设它可以让模型很容易  
地通过相对位置学习，因为对于任何固定的偏移量k，$PE_{pos+k}$都可以表示为$PEpos$的函数。  
我们还是用学习的位置嵌入进行了实验，发现两个版本产生了几乎相同的结果。见表3第（E）行。

我们选择正弦波模型是因为它可以让模型外推到比训练中遇到的序列长度更长的序列。

## 为什么是 Self-Attention

在本节中,我们比较了self-attention层的各个方面与经常用于将一个变长符号表示序列  
（x<sub>1</sub>,...,x<sub>n</sub>)映射到另一个等长序列(z<sub>1</sub>,...,z<sub>n</sub>)  
的循环层和卷积层，其中 x<sub>i</sub>,z<sub>i</sub>∈ R<sup>d</sup>,例如典型的序列  
转换编码器或解码器中的隐藏层。我们考虑了三个目标，激励我们使用self-attention。  

一个是每层的总计算复杂性。另一个是可以并行化的计算量，用所需的最小顺序操作数来衡量。  

第三个是网络中长期依赖项之间的路径长度。学习长期依赖性是许多序列转换任务中的一个  
关键挑战。影响学习这种依赖性能力的一个关键因素是，在网络中必须穿过的前向和后向信号的  
路径长度。输入序列和输出序列中任意位置组合之间的这些路径越短，学习长期依赖关系就越容易。  
因此，我们还比较了由不同层类型组成的网络中的任意两个输入和输出位置之间的最大路径长度。  

如表1所示，self-attention层将所有位置与一个常数的顺序执行操作连接起来，而一个循环层  
则需要O(n)的顺序操作。就计算复杂度而言，当序列长度n小于表示维数d时，self-attention层  
比循环层更快，这是机器翻译中最先进的模型（如单字和字节对儿表示）使用的句子表示最常见  
的情况。为了提高涉及非常长序列的任务的计算性能，可以将self-attention限制为仅考虑以  
各自输出位置为中心的输入序列中大小为r的领域。这将把最大路径长度增加到O(n/r)。  
我们计划在今后的工作中进一步研究这种方法。

核宽k < n的单个卷积层不连接所有输入和输出位置对。这样做需要一堆O(n/k)卷基层（对于  
连续的内核）或O(log<sub>k</sub>(n))（对于展开的卷积），增加网络中任意连个位置之间  
最长路径的长度。卷积层通常比循环层贵，系数为k。然而，可分离卷积将复杂度大大降低到  
O(k·n·d+n·d<sup>2</sup>)。然而，即使k=n，可分离卷积的复杂度也等于self-attention层  
和point-wise feed-forward层的组合，这是我们模型中采用的方法。

作为附带利益，self-attention可以产生更多可解释的模型。我们检查模型中的注意力分布，  
并在附录中给出并讨论示例。单个注意力头不仅能清楚地学习执行不同的任务，而且许多  
表现出与句子的句法和语义结构相关的行为。

## 5 训练

这一节描述了我们模型的训练方法。

## 5.1 训练数据和批

我们训练了标准的WMT 2014 English-German 数据集，包含大约450万个句子对。句子使用  
字节对儿编码进行编码，它的共享源目标词汇表大约有37000个标记。对于英语-法语，我们  
使用了更大的WMT 2014 English-French数据集，该数据集包含3600万句句子，并将标记拆分为  
32000个词条词汇。句子对儿按近似的序列长度分批在一起。每个训练批包含一组句子对，  
包含大约25000个源词和25000个目标词。

## 5.2 硬件和进度表

我们在一台机器上使用8个NVIDIA P100 GPUs对我们的模型进行了训练。对于使用本文中  
描述的超参数的基本模型，每个训练步骤大约需要0.4秒。我们对基础模型进行了100000步  
或12小时的训练。对于我们的大型模型（如表3的底线所述），步时间为1.0秒。大模型  
接受了300000步（3.5天）的训练。

## 5.3 优化

我们使用了Adam优化器，β<sub>1</sub>=0.9,β<sub>2</sub>=0.98,ϵ=10<sup>-9</sup>。  
根据公式，我们在整个训练过程中改变了学习率。

lrate = $d_{model}^{-0.5}·min(step_num^{-0.5},step_num·warmup_steps^{-1.5})$  

这对应与在第一个warmup_steps 训练steps中线性的增加学习率，然后与步数的平方成比例  
地减少学习率。我们使用 warmup_steps = 4000。

## 5.4 正则化

我们在训练中采用三种正则化方法。

**Residual Dropout** 我们将dropout应用于每个子层的输出，然后将其添加到子层输入  
并进行正则化。我们还将dropout应用于编码器和解码器堆栈中嵌入和位置编码的总和。  
对于基本的模型，我们使用a rate of P<sub>drop</sub> = 0.1。

表2：在English-to-German 和 English-to-French的2014年新闻测试中，Transformer比  
以前最先进的模型在一小部分的训练成本上获得了更高的BLEU分数。

|Model            |BLEU EN-DE|BLEU EN-FR|Training Cost(FLOPs)EN-DE|Training Cost(FLOPs)EN-FR|
|-----------------|-----|-----|-----|-----|
|ByteNet          |23.75|-|-|-|
|Deep-Att + PosUnk|-|39.2|-|$1.0·10^{20}$|
|GNMT + RL        |24.6|39.92|$2.3·10^{19}$|$1.4·10^{20}$|
|ConvS2S          |25.16|40.46|$9.6·10^{18}$|$1.5·10^{20}$|
|MoE              |26.03|40.56|$2.0·10^{19}$|$1.2·10^{20}$|
|-----------------|-----|-----|-----|-----|
|Deep-Att + PosUnk Ensemble|-|40.4|-|$8.0·10^{20}$|
|GNMT + RL Ensemble        |26.30|41.16|$1.8·10^{20}$|$1.1·10^{21}$|
|ConvS2S Ensemble          |26.36|**41.29**|$7.7·10^{19}$|$1.2·10^{21}$|
|-----------------|-----|-----|-----|-----|
|Transformer(base model)|27.3|38.1|$3.3·10^{18}$|**$3.3·10^{18}$**|
|Transformer(big)       |**28.4**|**41.8**|$2.3·10^{19}$|$2.3·10^{19}$|
|-----------------|-----|-----|-----|-----|

**Label Smoothing** 在训练过程中，我们采用了标签平滑值$ϵ_{ls}$ = 0.1。这会有损于  
perplexity，因为模型学了更不确定， 但提高了准确性和BLEU分数。

## 6 结果

## 6.1 机器翻译

在WMT 2014 English-to-German 翻译任务上，大的Transformer模型（表2中的Transformer  
（big）比之前报告的最高模型（包括集成）好2.0BLEU，建立了一个最先进的BLEU分数28.4。  
模型的配置列在表3的底线中，训练时间是3.5天在8个P100 GPUs上。即使是我们的基础模型  
也超过了以前发布的所有模型和集成，只是任何有竞争力模型训练成本的一小部分。  

在WMT 2014 English-to-French 翻译任务，我们的大模型BLEU分数达到41.0，超过了之前发布  
的所有单一模型，低于之前最先进模型训练成本的1/4。为English-to-French训练的Transformer  
（大）模型使用了dropout rate P<sub>drop</sub>=0.1,而不是0.3

对于基本模型，我们使用了一个单一的模型，该模型通过平均最后五个检查点获得，这些  
检查点以10分钟的间隔写入。对于大模型，我们平均了最后20个检查点。我们使用beam search  
beam 大小为4， 长度惩罚α = 0.6。这些超参数是在对开发集进行试验后选择的。 我们将  
推理期间的最大输出长度设置为输入长度+50，但在可能的情况下提前终止。  
表2总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较。  
我们通过乘以训练时间，使用的GPU数量和每个GPU的持续单精度浮点容量来估计用于训练模型的  
浮点操作数。

## 6.2 模型变化

为了评估Transformer不同组件的重要性，我们以不同的方式改变了我们的基础模型，  
测量了开发集中的English-to-German 翻译性能的变化，newstest2013。 如前一节所述，  
我们使用了beam search，但没有平均检查点。 我们在表3中给出了这些结果。在表3第（A）行，  
我们改变了注意力头的数量，键和值的维数，保持计算量不变，如3.2.2节中所述。  
虽然单头注意力比最佳设置差0.9BLEU,但是过多的头质量也会下降。

表3： Transformer 结构的变化。未列出的值与基本模型的值相同。所有的指标都在  
English-to-German翻译开发集上，newstest2013。 根据我们的byte-pair encoding，  
列出的perplexities 是per-wordpiece，不应于per-word perplexities进行比较。

||N|$d_{model}$|$d_{ff}$|h|$d_k$|$d_v$|$P_{drop}$|$ϵ_{ls}$|train steps|PPL(dev)|BLEU(dev)|params×10<sup>6</sup> |
|--|---|----|----|----|-----|-----|-----|-----|-----|------|------------|------------|
|base|6|512|2048|8|64|64|0.1|0.1|100K|4.92|25.8|65|
|--|--|--|--|--|--|--|--|--|--|--|--|--|
|(A)|--|--|--|1|512|512|--|--|--|5.29|24.9|--|
|(A)|--|--|--|4|128|128|--|--|--|5.00|25.5|--|
|(A)|--|--|--|16|32|32|--|--|--|4.91|25.8|--|
|(A)|--|--|--|32|16|16|--|--|--|5.01|25.4|--|
|--|--|--|--|--|--|--|--|--|--|--|--|--|
|(B)|--|--|--|--|16|--|--|--|--|5.16|25.1|58|
|(B)|--|--|--|--|32|--|--|--|--|5.01|25.4|60|
|--|--|--|--|--|--|--|--|--|--|--|--|--|
|(C)|2|--|--|--|--|--|--|--|--|6.11|23.7|36|
|(C)|4|--|--|--|--|--|--|--|--|5.19|25.3|50|
|(C)|8|--|--|--|--|--|--|--|--|4.88|25.5|80|
|(C)|--|256|--|--|32|32|--|--|--|5.75|24.5|28|
|(C)|--|1024|--|--|128|128|--|--|--|4.66|26.0|168|
|(C)|--|--|1024|--|--|--|--|--|--|5.12|25.4|53|
|(C)|--|--|4096|--|--|--|--|--|--|4.75|26.2|90|
|--|--|--|--|--|--|--|--|--|--|--|--|--|
|(D)|--|--|--|--|--|--|0.0|--|--|5.77|24.6||
|(D)|--|--|--|--|--|--|0.2|--|--|4.95|25.5||
|(D)|--|--|--|--|--|--|--|0.0|--|4.67|25.3||
|(D)|--|--|--|--|--|--|--|0.2|--|5.47|25.7||
|--|--|--|--|--|--|--|--|--|--|--|--|--|
|(E)|positional|embeddings|instead|of|sinusoids|--|--|--|--|4.92|25.7||
|--|--|--|--|--|--|--|--|--|--|--|--|--|
|big|6|1024|4096|6|--|--|0.3|--|300K|**4.33**|**26.4**|213|
|--|--|--|--|--|--|--|--|--|--|--|--|--|


在表3（B）行中我们观察到减小注意力键尺寸d<sub>k</sub>有损模型质量。这表明，确定 
兼容性并不容易，比dot product更复杂的兼容函数可能是有益的。我们在（C）和（D）中  
进一步观察到，正如预期的那样，较大的模型会更好，而dropout对于过拟合非常有帮助。  
在第（E）行中，我们将正弦位置编码替换为学习的位置嵌入，并观察到与基本模型几乎是  
相同的结果。

## 6.3 英语选区分析

表:4 Transformer 很好的泛化了英语选区分析（结果参见第23节WSJ）

|**Parser**|**Training**|**WSJ 23 F1**|
|----------|------------|-------------|
|Vinyals & Kaiser el al.(2014)|WSJ only,discriminative|88.4|
|Petrov el al.(2006)|WSJ only,discriminative|90.4|
|Zhu el al.(2013)|WSJ only,discriminative|90.4|
|Dyer el al.(2016)|WSJ only,discriminative|91.7|
|----------|------------|-------------|
|Transformer(4 layers)|WSJ only,discriminative|91.3|
|----------|------------|-------------|
|Zhu et al.(2013)|semi-supervised|91.3|
|Huang & Harper(2009)|semi-supervised|91.3|
|McClosky et al.(2006)|semi-supervised|92.1|
|Vinyals & Kaiser et al.(2014)|semi-supervised|92.1|
|----------|------------|-------------|
|Transformer (4 layers)|semi-supervised|92.7|
|----------|------------|-------------|
|Luong et al.(2015)|multi-task|93.0|
|Dyer et al.(2016)|generative|93.3|
|----------|------------|-------------|

为了评估Transformer是否可以推广到其他任务，我们对英语选区分析进行了实验。这项  
任务提出了具体的挑战：输出受到强大的结构约束，并且明显长于输入。此外，RNN序列到  
序列的模型在小数据环境中无法获取最先进的结果。 我们在Penn Treebank的Wall Street  
Journal（WSJ)上训练了一个4层的Transformer，其模型大小为d<sub>model</sub>=1024,  
大约有40K个训练句子。我们也在一个半监督的环境中训练它，使用更高信心和Berkleyparser  
语料库，大约有1700万句话。我们只在WSJ设置中使用了16K词的词汇表，在半监督设置中使用  
32K词的词汇表。我们值做了少量的实验来选择第22节开发集上的dropout，attention和  
residual（第5.4节）学习率和beam 大小，所有其他参数从与English-to-German的基本翻译  
模型保持不变。在推理过程中，我们将最大输出长度增加到输入长度+300。我们仅对WSJ和  
半监督设置使用beam大小21和α = 0.3。

我们在表4中的结果表明，尽管缺乏特定于任务的调整，我们的模型仍然运行的非常好，产生的  
结果比以前报告的所有模型都好，除了Recurrent Neural Network Grammer。 与RNN序列到序列  
模型相比，Transformer的性能优于Berkeley-Parser，即使它只训练由40K句组成的WSJ训练集。

## 7 结论

在这项工作中，我们提出了完全基于注意力的序列转换模型Transformer，用multi-headed  
self-attentin取代了编码器-解码器体系结构中最常用的循环层。

对于翻译任务，Transformer可以比基于循环层或卷积层的体系结构更快的训练。在 WMT 2014  
English-to-German  和 WMT 2014 English-to-French 翻译任务中，我们都获得了最先进的结果。  
在前一项任务中，我们的最好模型甚至比以前报道的所有集成模型都要出色。

我们对基于注意力的模型的未来感到兴奋，并计划将其应用到其他任务中去。 我们计划将  
Transformer扩展到涉及输入和输出模式（文本除外）的问题，并研究局部，受限注意力机制，  
以有效的处理图像，音频，和视频等大型输入和输出。另一个我们研究的目标是减少生成顺序。  

我们训练和评估的代码在 https://github.com/tensorflow/tensor2tensor

**鸣谢**
    我们非常感谢 Nal Kalchbrenner 和Stephan Gouws 的成果丰富的评注，更正和灵感。
