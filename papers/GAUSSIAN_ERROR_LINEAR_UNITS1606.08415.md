# 高斯误差线性单元

## 摘要

我们提出高斯误差线性单元（GELU)，一个高性能的神经网络激活函数。GELU的非线  
性是通过随机地应用恒等或0来映射一个神经网络的输入的随机正则化的预期转换。  
GELU的非线性权重输入通它们的量级而不是像ReLU那样通过输入的符号控制输入。我们  
执行一个关于GELU,ReLU和ELU三个激活函数的非线性能力对比的实验评估，发现性能的  
提升包括考虑的计算机视觉，自然语言处理和语音任务。  

## 1 介绍

早期的人工神经元利用二元阈值单元。这些硬二元决策被平滑化通过使用sigmoid激  
活函数,sigmoid函数使神经元能够拥有一个触发率的解释和跟反向传播一起训练.但是  
随着网络变得更深，带着sigmoid激活函数的训练被证明相比于做了一个基于输入符号  
的硬限制决策的非平滑更不太可能的ReLU更没有效率.尽管没有统计的激励，ReLU依然  
是一个具有竞争力的工程解决方案，它通常能比sigmoid更快更好的收敛。以ReLU的成  
功为基础，一个最近的改进被称为ELUs,它允许一个类ReLU的非线性输出负值，这有时  
能增加训练速度。总之，激活函数的选择仍然是神经网络的一项必要的结构决策，以  
免网络沦为一个深度线性分类器。  

深度非线性分类器可以把它们的数据拟合的太好,所以网络的设计者经常面对选择  
包含随机正则化像增加噪声到隐藏层或者应用dropout，而且这个选择依然区别于激活  
函数。 一些随机正则器能够使网络的行为像一个集成网络，一种伪集成，而且这个还  
能够导致标记准确率增长。例如，随机正则器dropout创建一个伪集成经过随机改变一  
些激活决策凭借与0相乘。非线性和dropout因此一起决定了一个神经元的输出,然而这  
两项创新仍然是不同的。更重要的是两者都没有包涵另一个， 因为流行的随机正则器  
跟输入行为无关而且非线性有助于这样的正则器。  

在这项工作中，我们提出一个新的非线性，高斯误差线性单元（GELU）。 它涉及  
到随机正则器对自适应dropout的改进的期望.  这暗示神经元输出的更具概率的观点。  
我们发现这个新奇的非线性匹敌或超过带着ReLU或ELUs的模型，所跨任务包括计算机视  
觉，自然语言处理，自动语音识别。  

## GELU 公式化

我们通过结合dropout，zoneout 和ReLU的属性来诱导我们的激活函数。首先注意  
到ReLU和dropout两者都能生成一个神经元输出，ReLU确切的用0或1乘以输入，  
而dropout随机的乘0.同样一个新的RNN正则器被叫做zoneout随机的用1乘以输入。 我  
们合并这个函数能力通过使用0或1乘以输入。但是这个0-1选择的值是随机决定的，这  
也依赖于输入。 具体的，我们可以用神经元输入x 乘以 m～Bernoulli（Φ(x)), 其中  
 Φ(x) = P(X ≤ x), X ∼ N (0, 1)  
这是一个标准正态分布的的积累分布函数。我们选择这个分布因为神经元输入倾向于  
跟着一个标准正态分布，尤其的，带着Batch Normalization。 在这个设置中，随着x  
的降低,输入有一个更高的概率被dorpped。所以转换应用到x上是随机的而且依赖于输  
入。 这种方式屏蔽输入保持不确定性，但是保持了对输入的依赖性。随机选择蒙蔽相  
当于对输入的随机零或者恒等变换。这非常像Adaptive Dropout，但是自适应dropout  
与非线性串联使用，并使用逻辑非标准正态分布。我们发现仅适用这个随机正则器，  
完全不使用其他非线性，有能力训练一个有竞争力的MNIST和TIMIT网络。  

我们经常想要一个确定性的决定从一个神经网络，而且这个给出了我们的新非线性。  
非线性随机正则化的预期变换在一个输入x上，即：  
    Φ(x) * Ix + (1 − Φ(x)) * 0x = xΦ(x)  
不严格的说，这个表达式表示我们按x比其他输入大多少来缩放x。 因为高斯的积累分布  
函数通常是用误差函数计算的，我们定义高斯误差线性单元（GELU）为：  
    GELU(x) = xP(X ≤ x) = xΦ(x)  
我们可以近似GELU用： 0.5x(1 + tanh[ 2/π(x + 0.044715x3)]) 或者 xσ(1.702x)  

我们会使用 N (μ, σ2) 的 CDF 而且让 μ 和 σ 成为可以被学习的超参数，但是贯穿  
这个工作，我们只是让μ = 0 and σ = 1 。所以我们不会引入任何新的超参数在下面的实  
验中。在下一节中，我们展示了在许多任务中，GELU超过了ReLU和ELUs的性能。  

## 3 GELU 实验

我们评估了GELU，ELU 和ReLU在:  
    MNIST分类  
    MNIST 自动编码  
    Tweet 词性标注  
    TIMIT 帧识别  
    CIFAR－10/100 分类  
我们并没有评估非线性像LReLU，因为它跟ReLU很像。  

实验以及实验结果略过。

## 4 讨论

在几个实验中，GELU 比先前的非线性函数更好，但是在其他方面它也与ReLU和ELU  
有相似之处。例如，当σ → 0 和如果 μ = 0， GELU 便成了ReLU。甚至，ReLU和GELU是  
等渐进的。事实上，GELU 可以被看为一种平滑ReLU的方式。看到这个，回想  
ReLU ＝ max(x, 0) = x1(x > 0) （1是指示器函数）  
然而 GELU 是 xΦ(x)， 如果 μ = 0, σ = 1。 那么CDF是ReLU的二元函数的平滑近似值。  
就像sigmoid平滑二元阈值激活函数一样。不像ReLU，GELU和ELU都能取正值和负值。  
事实上，如果我们使用标准的柯西分布的积累分布函数，那么ELU（当 α = 1/π) 渐进等于  
xP(C ≤ x),C ∼ Cauchy(0,1)  
对于负值和正值，如果我们向下移动1/π， 则为xP(C ≤ x)。这些是与先前非线性函数的基本关系。  

然而，GELU有一些值得注意的不同之处。这个非凸，非单调函数在正域中不是线性的，  
而且在所有点上都有曲率。同时，ReLU和ELUs 是在正域中的凸单调激活函数，因此可能  
缺少曲率。 因此，增加的曲率和非单调性可能使GELU比ReLU和ELUs更容易近似复杂函数。  
另外,由于 ReLU(x) = x1(x > 0) 和 GELU(x) = xΦ(x) 如果μ = 0, σ = 1,  
我们可以看到，ReLU根据其符号对输入进行门控。而GELU根据输入比其他输入大多少来  
对输入进行加权。此外，值得注意的是，GELU有给出一个概率化的解释即随机正则化的期望.  

我们也有连个使用GELU的实用技巧。首先，我们建议使用GELU训练时使用momentum优化  
器，这是深度神经网络的一个标准。其次，使用高斯分布积累分布函数的近似值很重要。  
一个sigomid函数 σ(x) = 1/(1 + e−x) 是正态分布的累积函数的近似值。  
然而我们发现Sigmoid 线性单元（SiLU） xσ(x), 表现的更糟糕比GELU，但是通常会比  
ReLU和ELUs更好。我们使用 0.5x(1 + tanh[ 2/π(x + 0.044715x3)]) 或者 xσ(1.702x)  
而不是使用 xσ(x) 来近似 Φ(x). 这两种方法都具有足够的快速性和易于实现的近似性，  
在本文的每个实验中我们都使用了前者。  

## 5 结论

对于本文所评估的众多数据集，GELU始终超过ELU和ReLU的准确率，使其成为以前  
的非线性激活函数的可行替代方案。
